# Project
mode: eval

seed: 42

# Dataset or file
data: open_natural_question
test_file: ./data/dataset/{data}/nq_test.jsonl

# Cache dir
# cache_dir: ./output/eval/{data}_long/
# cache_dir: ./output/eval/{data}/best/
cache_dir: ./output/{data}/2023-11-10-21-25-01/

# Output dir. Output will be saved into "output_dir/%Y_%m_%d_%H_%M_%S/{rank}"
output_dir: ./output/{data}/

# Result will be saved in output_dir/result_{info_}file/
result_file: result.json
result_info_file: result_info.json

# gpu id list. If set None, will use gpu in [0, torch.cuda.device_count() - 1]
gpu: 0, 1, 2, 3

# torch.DistributeDataParallel
ddp: True

# LLM Config
LLMConfig:
  llm1:
    model_name: llama2_base_reward_model
    model_path: ./data/model/llama2_base_reward_model/pytorch_model.bin
    model_class: LlamaModel
    fp16: True
    tokenizer_class: LlamaTokenizer
  llm2:
    model_name: llama2-chat
    model_path: daryl149/llama-2-7b-chat-hf
    model_class: LlamaForCausalLM
    fp16: True
    tokenizer_class: LlamaTokenizer
    padding_side: left
  llm3:
    model_name: xverse-chat
    model_path: xverse/XVERSE-13B-Chat
    model_class: LlamaForCausalLM
    fp16: True
    tokenizer_class: LlamaTokenizer
  llm4:
    model_name: xverse_base_reward_model
    model_path: ./data/model/xverse_base_reward_model/pytorch_model.bin
    model_class: LlamaModel
    fp16: True
    tokenizer_class: AutoTokenizer
  llm5:
    model_name: en_query_encoder
    model_path: ./data/model/webglm_dual_encoder/query_encoder
    model_class: AutoModel
    fp16: False
    tokenizer_path: facebook/contriever-msmarco
    tokenizer_class: AutoTokenizer
  llm6:
    model_name: en_paragraph_encoder
    model_path: ./data/model/webglm_dual_encoder/paragraph_encoder
    model_class: AutoModel
    fp16: False
    tokenizer_path: facebook/contriever-msmarco
    tokenizer_class: AutoTokenizer
  llm7:
    model_name: chatglm2
    model_path: THUDM/chatglm2-6b
    model_class: AutoModel
    fp16: True
    tokenizer_class: AutoTokenizer
    padding_side: left
  llm8:
    model_name: bert-base-chinese
    model_path: bert-base-chinese
    model_class: BertModel
    fp16: False
    tokenizer_class: AutoTokenizer
  llm9:
    model_name: deberta
    model_path: microsoft/deberta-xlarge-mnli
    model_class: AutoModel
    fp16: False
    tokenizer_class: AutoTokenizer
  llm10:
    model_name: zh_encoder
    model_path: BAAI/bge-large-zh
    model_class: AutoModel
    fp16: False
    tokenizer_class: AutoTokenizer
  llm11:
    model_name: huozi-rlhf
    model_path: HIT-SCIR/huozi-7b-rlhf
    model_class: AutoModelForCausalLM
    fp16: True
    tokenizer_class: AutoTokenizer
    padding_side: left

# Will initial and release after usage. 
LLMMap: 
  gpu0: 


# Modules
Module:
  Retrieval: Wiki, Web, Gendoc
  Knowledge: Summarizer, Contriver
  Response: Generator
  Evaluate: Voter, Scorer

# Method config
ModuleConfig:
  Web:
    ban: False
    load_result_file: web_doc.json
    save_result_file: web_doc.json
    load_info_file: web_info.json
    save_info_file: web_info.json
    log_detail: False
    ssl_verify: False
    min_doc_len: 50
    max_doc_len: 1000


  Wiki:
    ban: False
    load_result_file: wiki_doc.json
    save_result_file: wiki_doc.json
    load_info_file: wiki_info.json
    save_info_file: wiki_info.json
    log_detail: False
    tokenize_kwargs:
      padding: "longest"
      truncation: True
      max_length: 128
    encode_kwargs:
      batch_size: 1
      pooling_method: cls


  Gendoc:
    ban: False
    load_result_file: gen_doc.json
    save_result_file: gen_doc.json
    load_info_file: gendoc_info.json
    save_info_file: gendoc_info.json
    log_detail: False
    zh_model_name: chatglm2
    zh_template_id: 1
    en_model_name: llama2-chat
    en_template_id: 4
    tokenize_kwargs:
      padding: "longest"
      truncation: True
    generate_kwargs:
      batch_size: 16
      temperature: 0.4
      top_p: 0.9
      top_k: 50
      repetition_penalty: 1.1
      do_sample: True


  Summarizer:
    ban: False
    load_result_file: summarize_result.json
    save_result_file: summarize_result.json
    load_info_file: summarize_info.json
    save_info_file: summarize_info.json
    zh_model_name: chatglm2
    zh_template_id: 4
    en_model_name: llama2-chat
    en_template_id: 4
    tokenize_kwargs:
      padding: "longest"
      truncation: True
    generate_kwargs:
      batch_size: 16
      temperature: 0.4
      top_p: 0.9
      top_k: 50
      repetition_penalty: 1.1
      do_sample: False


  Contriver:
    ban: False
    load_result_file: contrive_result.json
    save_result_file: contrive_result.json
    load_info_file: contrive_info.json
    save_info_file: contrive_info.json
    min_knowledge_len: 300
    tokenize_kwargs:
      padding: "longest"
      truncation: True
      max_length: 512
    encode_kwargs:
      batch_size: 24
      pooling_method: mean


  Generator:
    ban: False
    load_result_file: response_result.json
    save_result_file: response_result.json
    load_info_file: response_info.json
    save_info_file: response_info.json
    zh_model_name: huozi-rlhf
    zh_template_id: 3
    en_model_name: huozi-rlhf
    en_template_id: 3
    tokenize_kwargs:
      padding: "longest"
      truncation: True
    generate_kwargs:
      batch_size: 16
      temperature: 0.4
      top_p: 0.9
      top_k: 50
      repetition_penalty: 1.2
      do_sample: False
      max_new_tokens: 300

  Voter: #default module for voting, DO NOT change the module name
    ban: False
    type: Voter
    load_result_file: 
    save_result_file: vote_result.json
    load_info_file: 
    save_info_file: vote_info.json
    scoring_method: bertscore #one of nli, bertscore, rarebertscore, em, f1, nli_with_query or composition. Default is nli
    bidirectional: # if True, will use (score(s_i, s_j) + score(s_j. s_i))/2 for similarity score, this is only applicable for asymmetric similarity score: [nli, nli_with_query, em] 
      nli: True
      nli_with_query: True
      em: True
    composition_weight: [1, 1, 1, 1, 1, 1] # if voting method is weight, final score with be the weighted sum, weight is given by the normlized composition weight. each number is the weight of "nli","bertscore","rarebertscore","em","f1","nli_with_query"
    threshold: 0 # threshold to filter out answer, default is 0.5
    pooling_method: topk # should be one of max, mean, topk, voting or majority voting, default is max
    pooling_threshold: # threshold to filter out unsimilar answers in pooling methods
    min_acceptance_num:  2 #when pooling_method is voting, only answer with more than 'min_accepatance_num' similar answers which similarity score > threshold will be kept. if None, min_acceptance_num will be set to math.ceil(num/2)
    mean_pooling_topk: 3 #if pooling method is topk, will only take the average of the top 'mean_pooling_topk' number of scores, default is math.ceil(num/2)
    batch_size: 64
    hierarchical_voting: 
      turn_on: True # if True and 'num_responses_per_prompt' in response modules is bigger than 1, then voting module will firstly vote within each query_knowledge pair(because each query-knowledge pair have more than 1 responses), then voting within each query, if False, will directly vote within each query.
      pooling_method: majority_voting
      pooling_threshold: 
      min_acceptance_num:  2
      mean_pooling_topk:

  Scorer:
    ban: False
    load_result_file: 
    save_result_file: score_result.json
    load_info_file: 
    save_info_file: score_info.json
    zh_model_name: xverse_base_reward_model
    zh_template_id: 1
    en_model_name: llama2_base_reward_model
    en_template_id: 1
    tokenize_kwargs: 
      padding: "longest"
      truncation: True
      max_length: 1024
    reward_kwargs:
      batch_size: 24
